{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2c67bc-71f6-4f9b-aa02-aa4053fca962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8230c2b6-a65b-49c0-a7f5-2336bcb6d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4942cee-fbb3-40d5-a73a-f27ca2f1e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86c2a083-29a1-42e5-8eec-52a87a2395d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buck Shlegeris is a Research Scientist at OpenAI focused on long-term safety in AI systems. He has been involved in researching various aspects of AI safety and has made valuable contributions to the field. Although he may not have specifically worked on iterated distillation and amplification (IDA) papers, his research and insights are still relevant to AGI alignment and safety. His work indirectly plays a role in shaping the broader landscape of AI safety research, which includes IDA as one of the approaches.\n"
     ]
    }
   ],
   "source": [
    "input_content = \"\"\"\n",
    "Mitchell: \n",
    "Conditional on AGI being developed by 2070, what is the probability that humanity will suffer an existential catastrophe due to loss of control over an AGI system?\n",
    "\n",
    "GPT4: \n",
    "The risks associated with AGI development will depend on factors such as safety precautions, governance, international cooperation, and broader awareness of possible risks. The best approach is to work on developing AI safety research, responsible AI policies and global cooperation to ensure that AGI is built in a manner that is beneficial to humanity and mitigates the risks of existential catastrophe.\n",
    "\n",
    "Mitchell: \n",
    "What are the inputs into technical AGI safety alignment? And what are the most plausible approaches to solving AGI alignment?\n",
    "\n",
    "GPT4: \n",
    "Inputs into technical AGI safety alignment generally involve understanding and defining what kind of behavior we want the AGI to exhibit, ensuring that its actions are aligned with human values, and developing methods to prevent misaligned behavior. Some inputs into AGI alignment include:\n",
    "\n",
    "1. Value specification: Defining appropriate objective functions, goals, or reward signals for AGI that encompass human values and alignment with long-term human interests.\n",
    "2. Robustness: Ensuring the AGI system remains stable, correct, and aligned under various conditions, including uncertainty, limited information, or adversarial attacks.\n",
    "3. Interpretability: Building AGI systems that are transparent, explainable, and understandable by humans, helping us discern if and when the system is misaligned.\n",
    "4. Scalability: Developing AGI safety solutions that can scale with the increasing complexity and capabilities of AI systems.\n",
    "5. Continuous improvement: Ensuring that AGI systems can learn and adapt over time while maintaining alignment, avoiding value drift or undesirable extrapolation of initial objectives.\n",
    "\n",
    "Some plausible approaches to solving AGI alignment include:\n",
    "\n",
    "1. Inverse reinforcement learning (IRL): Learning a reward function from observed human behavior and then training the AGI system to optimize for that reward signal. This approach seeks to infer human values through observation and may be combined with other techniques to refine the reward function further.\n",
    "2. Cooperative inverse reinforcement learning (CIRL): Casting the alignment problem as a cooperative game between the AGI and human, wherein both are trying to maximize a shared reward function. This approach emphasizes the importance of the AGI system learning to cooperate with humans and other AGI systems.\n",
    "3. Debate and amplification: Using AI systems to simulate debates, in which different agents argue for possible actions or policies, with human judges selecting the most convincing arguments. This could help surface potential risks and avoid deceptive behavior from AGI systems.\n",
    "4. Iterated distillation and amplification (IDA): Training AGI systems to perform tasks by iteratively breaking down complex tasks into simpler sub-tasks, and then using a combination of AI assistance and human feedback to improve the AI system's understanding of human values and its responses to alignment questions.\n",
    "5. Value learning: Developing AGI systems that can infer and learn human values from different sources, including human feedback, natural language text, and other cultural artifacts. This approach aims to implicitly embed human preferences into AGI systems.\n",
    "\n",
    "Collaborative efforts among AI researchers, ethicists, policymakers, and other stakeholders will be necessary to develop and test these approaches' effectiveness in ensuring AGI aligns with human values and interests.\n",
    "\n",
    "Mitchell: \n",
    "Of those 5 plausible approaches, please rank them from most promising to least promising where 1 is the most promising. Also, give your reasoning why that type of research shows promise and possibly where to go next.\n",
    "\n",
    "GPT4:\n",
    "It is difficult to definitively rank these approaches, as each has its strengths and limitations, and the most promising long-term AGI alignment strategy may involve a combination of these techniques. However, I can provide an analysis of their current promise, noting that the effectiveness of these approaches may evolve over time as further research is undertaken. Please keep in mind that this is a subjective assessment.\n",
    "\n",
    "1. Iterated distillation and amplification (IDA): IDA shows promise as it aims to harness the combined strengths of AI assistance and human feedback. By breaking down complex tasks into simpler sub-tasks and refining the AI's understanding of human values, IDA may enable AGI systems to understand and learn from nuanced feedback. Continued research in this area might involve refining the methods used to break down tasks and improving methods of providing human feedback to AI models.\n",
    "\n",
    "2. Inverse reinforcement learning (IRL) and Cooperative inverse reinforcement learning (CIRL): Both IRL and CIRL are promising because they focus on learning a reward function from observed human behavior or direct cooperation with humans. Both of these approaches emphasize inferring human values and adapting AGI systems to optimize for those values. Future research can investigate how to obtain more accurate reward functions or develop more efficient learning algorithms to ensure AGI alignment.\n",
    "\n",
    "3. Value learning: This approach is promising because it aims to leverage various sources of human value information, including text, cultural artifacts, and human feedback. These diverse sources may help create a more robust understanding of human preferences and values. Future directions for value learning might involve improving the methods used to extract value information from diverse sources, and further investigating how to ensure AGI systems adequately model this information.\n",
    "\n",
    "4. Debate and amplification: This approach can potentially help identify deceptive AGI behavior and uncover hidden risks through argumentation. However, there may be some concerns regarding the scalability and limitations of relying on human judges for debate outcomes. Nonetheless, investigating the methods used to simulate debates and enhance human judges' decision-making could be fruitful avenues for future research.\n",
    "\n",
    "It's important to note that this subjective ranking should not discourage research in any of these areas. The field of AGI safety is complex and rapidly evolving. To ensure the development of safe and beneficial AGI, a combination of approaches may be necessary, and substantial progress in one area may change the perceived promise of these techniques over time. Further research, development, and collaboration among different stakeholders will be critical to determining the most effective AGI safety alignment methods.\n",
    "\n",
    "Mitchell: \n",
    "Please give me all the title references for Iterated distillation and amplification (IDA) papers. Also give a list of all the researchers who've contributed to IDA.\n",
    "\n",
    "GPT4:\n",
    "Iterated distillation and amplification (IDA) is a relatively new concept, and the number of papers directly addressing IDA is limited. However, the primary source of the concept can be found in the following paper:\n",
    "\n",
    "1. Christiano, P. (2018). \"Supervising strong learners by amplifying weak experts.\" AI Alignment Blog. Retrieved from https://ai-alignment.com/research-ideas-964639a853ea\n",
    "\n",
    "Although the author, Paul Christiano, is credited with the introduction of iterated distillation and amplification, many researchers have contributed to related topics such as AI alignment, amplification, and others that fall under the broader umbrella of AGI safety. Some of these researchers include:\n",
    "\n",
    "- Paul Christiano: Research Scientist at OpenAI\n",
    "- Stuart Russell: Professor of Computer Science at UC Berkeley\n",
    "- Ilya Sutskever: Co-founder and Chief Scientist of OpenAI\n",
    "- Dario Amodei: Director of Research at OpenAI\n",
    "- Jan Leike: Senior Research Scientist at DeepMind\n",
    "- Chris Olah: Research Scientist at OpenAI\n",
    "- Victoria Krakovna: Research Scientist at DeepMind\n",
    "- Owain Evans: Research Scientist at OpenAI\n",
    "- Tom Dietterich: Professor of Computer Science at Oregon State University\n",
    "\n",
    "Keep in mind that research in AI alignment, including IDA, is highly interdisciplinary, and the list of researchers above is by no means exhaustive. The field is continuously growing, and there are many researchers working on different aspects of AI safety and AGI alignment who may, directly or indirectly, contribute to the development of IDA and related approaches in the future.\n",
    "\n",
    "Mitchell: What about Buck Shlegeris?\n",
    "\n",
    "GPT4: Buck Shlegeris is a Research Scientist at OpenAI focused on long-term safety in AI systems. He has been involved in researching various aspects of AI safety and has made valuable contributions to the field. Although he may not have specifically worked on iterated distillation and amplification (IDA) papers, his research and insights are still relevant to AGI alignment and safety. His work indirectly plays a role in shaping the broader landscape of AI safety research, which includes IDA as one of the approaches.\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\", \n",
    "    messages=[{\"role\": \"user\", \n",
    "               \"content\": input_content}]\n",
    "    )\n",
    "string_to_copy = completion.choices[0].message.content\n",
    "pyperclip.copy(string_to_copy)\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140d833-4424-47a9-b28b-f2e82e3391c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30cd9d20-4a02-4d9e-96be-058ab91b9823",
   "metadata": {},
   "source": [
    "## LangChain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d576d33-218a-4af7-b1e2-d1d7753ef1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"math_func\"],\n",
    "    template=\"write a python function that outputs an S-curve {math_func} along with documentation on how to adjust the various parameters\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7d2bb9-f74f-4ebb-9d62-2c3dfd7439f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef sigmoid_scurve(slope, center_x, center_y):\\n    \"\"\"\\n    Returns a sigmoid S-curve\\n\\n    Parameters\\n    ----------\\n    slope : float\\n        The steepness of the curve\\n    center_x : float\\n        The x-coordinate of the center point of the curve\\n    center_y : float\\n        The y-coordinate of the center point of the curve\\n\\n    Returns\\n    -------\\n    sigmoid : function \\n        A function of the form y = 1 / (1 + exp(-slope * (x - center_x))) + center_y\\n    \"\"\"\\n    \\n    def sigmoid(x):\\n        return 1 / (1 + np.exp(-slope * (x - center_x))) + center_y\\n    \\n    return sigmoid'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d52539-b196-4085-a990-bf17ad8d16e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071edfd1-2bfb-4af3-9ce6-d9cde22f5f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55f437b-08b0-468f-a789-34ad8c7e1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0, \n",
    "                  model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0c5d9e-a024-499e-b0be-45bd19300040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='La inteligencia artificial es el futuro.', additional_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "     \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, \n",
    "                                                human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language=\"English\", \n",
    "                               output_language=\"Spanish\", \n",
    "                               text=\"Artificial Intelligence is the future.\").to_messages())\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2558e55-68ff-4395-87c9-83b1b521b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
